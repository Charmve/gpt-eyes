{
    "summary": "The code imports modules, handles events, and enables video capturing or prediction. It renders an image or video based on 'capturedImage' with options to retake or analyze. The analysis results are displayed in buttons for further interaction with GPT.",
    "details": [
        {
            "comment": "This code imports necessary modules and defines a function makeElementFullscreen which resizes an element based on the window size. It listens to 'resize' event and 'loadedmetadata' for video elements.",
            "location": "\"/media/root/Toshiba XG3/works/gpt-eyes/docs/src/src/App.js\":0-31",
            "content": "import React, { useEffect, useRef, useState } from 'react';\nimport { runObjectDetection } from './controller/ImageAnalyser';\nimport './App.css';\nimport { askGpt } from './controller/gpt';\nfunction makeElementFullscreen(element, elementType) {\n  function resizeElement() {\n      const windowWidth = window.innerWidth;\n      const windowHeight = window.innerHeight;\n      let elementAspectRatio;\n      if (elementType === 'image') {\n          elementAspectRatio = element.naturalWidth / element.naturalHeight;\n      } else if (elementType === 'video') {\n          elementAspectRatio = element.videoWidth / element.videoHeight;\n      }\n      const windowAspectRatio = windowWidth / windowHeight;\n      if (windowAspectRatio > elementAspectRatio) {\n          element.style.width = '100%';\n          element.style.height = 'auto';\n      } else {\n          element.style.width = 'auto';\n          element.style.height = '100%';\n      }\n  }\n  window.addEventListener('resize', resizeElement);\n  if (elementType === 'video') {\n      element.addEventListener('loadedmetadata', resizeElement);"
        },
        {
            "comment": "This code is initializing a video stream, enabling the user to record or analyze video input, and obtaining object detection predictions from an image.",
            "location": "\"/media/root/Toshiba XG3/works/gpt-eyes/docs/src/src/App.js\":32-66",
            "content": "  }\n  resizeElement();\n}\nconst App = () => {\n  const [isAnalysing, setAnalysing] = useState(false);\n  const videoRef = useRef(null);\n  const [isRecording, setIsRecording] = useState(true);\n  const [capturedImage, setCapturedImage] = useState(null);\n  const [predictions, setPredictions] = useState([]);\n  // const imageElement = document.getElementById('capture-img');\n  // makeElementFullscreen(imageElement, 'image');\n  // const videoElement = document.getElementById('capture-video');\n  // makeElementFullscreen(videoElement, 'video');\n  useEffect(() => {\n    const initWebcam = async () => {\n      try {\n        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' } });\n        if (videoRef.current) {\n          videoRef.current.srcObject = isRecording ? stream : null;\n        }\n      } catch (error) {\n        console.error('Error accessing webcam:', error);\n      }\n    };\n    initWebcam();\n  }, [isRecording]);\n  useEffect(() => {\n    const getPredictions = async () => {\n      const incomingPredictions = await runObjectDetection(capturedImage);"
        },
        {
            "comment": "This code appears to be part of a video capturing and prediction application. When a new image is captured, it calls the `getPredictions()` function to analyze the image and update the `incomingPredictions` state variable. The `setAnalysing(false)` line suggests that there may be some ongoing analysis process.\n\nIf an image has been captured (`capturedImage`), it triggers the prediction logic by calling `getPredictions()`.\nThe `takePicture()` function starts the image capturing process, creating a canvas to draw the video frame and converting it into a JPEG format. It then sets the `capturedImage` state variable and stops recording with `setIsRecording(false)`.\nThe `retakePicture()` function clears the captured image by setting `capturedImage` to null and resets the recording state with `setIsRecording(true)`.\nThe return statement suggests that there is a div element containing a video feed, and if an image has been captured (`capturedImage`), it displays that image.",
            "location": "\"/media/root/Toshiba XG3/works/gpt-eyes/docs/src/src/App.js\":67-99",
            "content": "      console.log(\"Predictions: \", incomingPredictions);\n      setPredictions(incomingPredictions);\n      setAnalysing(false);\n    };\n    if (capturedImage) {\n      getPredictions();\n    }\n  }, [capturedImage]);\n  const takePicture = () => {\n    setAnalysing(true);\n    const canvas = document.createElement('canvas');\n    canvas.width = videoRef.current.videoWidth;\n    canvas.height = videoRef.current.videoHeight;\n    const context = canvas.getContext('2d');\n    context.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);\n    const image = canvas.toDataURL('image/jpeg');\n    setCapturedImage(image);\n    setIsRecording(false);\n  };\n  const retakePicture = () => {\n    setCapturedImage(null);\n    setIsRecording(true);\n  };\n  return (\n    <div style={{ height: '100%', overflow: 'hidden' }}>\n      <div style={{ flex: '1', backgroundColor: 'black' }}>\n        <div style={{ width: '100%', position: 'relative' }}>\n          <div style={{ flex: '1', backgroundColor: 'black', position: 'relative' }}>\n            {capturedImage ? ("
        },
        {
            "comment": "This code is rendering either an image or a video, depending on whether 'capturedImage' exists or not. If the image exists, it displays the image and shows a button to retake the picture if 'isAnalysing' is false. If the image does not exist but 'isAnalysing' is true, it displays a loading spinner. The button to take a new picture is displayed when there is no captured image or analysis in progress.",
            "location": "\"/media/root/Toshiba XG3/works/gpt-eyes/docs/src/src/App.js\":100-114",
            "content": "              <img src={capturedImage} id=\"capture-img\" alt=\"Captured\" style={{ width: '100%', height: '100%' }} />\n            ) : (\n              <video ref={videoRef} id=\"capture-video\" autoPlay playsInline style={{ width: '100%', height: '100%' }}></video>\n            )}\n            <div style={{ position: 'fixed', bottom: '20px', right: '20px' }}>\n              { isAnalysing ? (\n                <div className=\"loading-spinner\">\n                  <div className=\"spinner\"></div>\n                </div>\n              ) : (capturedImage ? (\n                <button onClick={retakePicture} style={{ borderRadius: '360px', backgroundColor: 'red', color: 'white', width: '75px', height: '75px', padding: '0', display: 'flex', justifyContent: 'center', alignItems: 'center', border: '1px solid black' }}>\n                  Restart\n                </button>\n              ) : (\n                <button onClick={takePicture} style={{ borderRadius: '360px', backgroundColor: 'green', color: 'white', width: '75px', "
        },
        {
            "comment": "This code renders a button labeled \"Analyse\" and displays the results of an analysis in buttons that can be clicked to ask GPT for more information.",
            "location": "\"/media/root/Toshiba XG3/works/gpt-eyes/docs/src/src/App.js\":114-141",
            "content": "height: '75px', padding: '0', display: 'flex', justifyContent: 'center', alignItems: 'center', border: '1px solid black' }}>\n                  Analyse\n                </button>\n              ))}\n            </div>\n          </div>\n        </div>\n      </div>\n      <div style={{ flex: '1', padding: '20px', backgroundColor: '#fff', overflowY: 'auto' }}>\n        <div style={{ overflowX: 'scroll', whiteSpace: 'nowrap', height: '100%' }}>\n          <p>Analyse result:</p>\n          {predictions.map((message, index) => (\n            <button className='optionsBtn' type=\"button\" key={index} onClick={() => askGpt(message.className)}>\n              {message.className}\n            </button>\n          ))}\n        </div>\n      </div>\n    </div>\n  );\n};\nexport default App;"
        }
    ]
}